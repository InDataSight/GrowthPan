{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "3iJuNSZ2N0tK",
        "Sg1zYTEiPYrk"
      ],
      "authorship_tag": "ABX9TyMnZ+cqLzm4Ryoa0y2Hkj7c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/InDataSight/GrowthPan/blob/main/ETLconceptworkflow1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.0 Setup"
      ],
      "metadata": {
        "id": "3iJuNSZ2N0tK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Folder structure"
      ],
      "metadata": {
        "id": "1CzvuVMaOpNF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "aHUVrvZUydHy"
      },
      "outputs": [],
      "source": [
        "!mkdir ScrapyProfesiaRawData ScrapyProfesiaProcessedData ScrapyProfesiaLogs"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Install libraries and modules"
      ],
      "metadata": {
        "id": "I_oTZca_OwCe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define requirements\n",
        "requirements = \"\"\"bs4\n",
        "azure.storage.blob\n",
        "\"\"\"\n",
        "\n",
        "# Specify the file name\n",
        "file_name = \"requirements.txt\"\n",
        "\n",
        "# Write the text to the file\n",
        "with open(file_name, \"w\") as file:\n",
        "    file.write(requirements)\n",
        "\n",
        "print(f\"{file_name} created successfully with the following content:\\n{requirements}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUHQ55BlRFc-",
        "outputId": "7d5aa6e7-5767-4a69-ac78-8249ed617f78"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "requirements.txt created successfully with the following content:\n",
            "bs4\n",
            "azure.storage.blob\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "Ox5X4Ff3RwyK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb1ec597-8f8e-4b83-da60-30d12218d24c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bs4 (from -r requirements.txt (line 1))\n",
            "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
            "Collecting azure.storage.blob (from -r requirements.txt (line 2))\n",
            "  Downloading azure_storage_blob-12.24.1-py3-none-any.whl.metadata (26 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from bs4->-r requirements.txt (line 1)) (4.12.3)\n",
            "Collecting azure-core>=1.30.0 (from azure.storage.blob->-r requirements.txt (line 2))\n",
            "  Downloading azure_core-1.32.0-py3-none-any.whl.metadata (39 kB)\n",
            "Requirement already satisfied: cryptography>=2.1.4 in /usr/local/lib/python3.11/dist-packages (from azure.storage.blob->-r requirements.txt (line 2)) (43.0.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from azure.storage.blob->-r requirements.txt (line 2)) (4.12.2)\n",
            "Collecting isodate>=0.6.1 (from azure.storage.blob->-r requirements.txt (line 2))\n",
            "  Downloading isodate-0.7.2-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: requests>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from azure-core>=1.30.0->azure.storage.blob->-r requirements.txt (line 2)) (2.32.3)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from azure-core>=1.30.0->azure.storage.blob->-r requirements.txt (line 2)) (1.17.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=2.1.4->azure.storage.blob->-r requirements.txt (line 2)) (1.17.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->bs4->-r requirements.txt (line 1)) (2.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=2.1.4->azure.storage.blob->-r requirements.txt (line 2)) (2.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.21.0->azure-core>=1.30.0->azure.storage.blob->-r requirements.txt (line 2)) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.21.0->azure-core>=1.30.0->azure.storage.blob->-r requirements.txt (line 2)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.21.0->azure-core>=1.30.0->azure.storage.blob->-r requirements.txt (line 2)) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.21.0->azure-core>=1.30.0->azure.storage.blob->-r requirements.txt (line 2)) (2024.12.14)\n",
            "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
            "Downloading azure_storage_blob-12.24.1-py3-none-any.whl (408 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m408.4/408.4 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_core-1.32.0-py3-none-any.whl (198 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.9/198.9 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading isodate-0.7.2-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: isodate, bs4, azure-core, azure.storage.blob\n",
            "Successfully installed azure-core-1.32.0 azure.storage.blob-12.24.1 bs4-0.0.2 isodate-0.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install azure.storage.blob\n",
        "#!pip install azure.identity"
      ],
      "metadata": {
        "id": "RTDSTs76glS2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "import re\n",
        "import datetime\n",
        "\n",
        "#Run pip for defined modules in the requirements.txt\n",
        "pip_install_result = subprocess.run(['pip', 'install', '-r', 'requirements.txt'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "\n",
        "#output results for the pip_install_result\n",
        "if pip_install_result.returncode == 0:\n",
        "    print(\"Pip install successful\")\n",
        "else:\n",
        "    print(\"Pip install failed\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eormRimU9LXq",
        "outputId": "2e9644e4-c6b9-401a-cbd5-00516a120d14"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pip install successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3 Test"
      ],
      "metadata": {
        "id": "SAgf05ZNOzBe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define unit testing\n",
        "requirements = \"\"\"def test_library_installation():\n",
        "    try:\n",
        "        import requests, google.colab\n",
        "    except ImportError:\n",
        "        assert False, \"Required library is not installed\"\n",
        "    assert True\n",
        "\"\"\"\n",
        "\n",
        "# Specify the file name\n",
        "file_name = \"ScrapyProfesiaSetupTest.py\"\n",
        "\n",
        "# Write the text to the file\n",
        "with open(file_name, \"w\") as file:\n",
        "    file.write(requirements)\n",
        "\n",
        "print(f\"{file_name} created successfully with the following content:\\n{requirements}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txhe0qZ6m6qk",
        "outputId": "bc661a92-ca87-4b95-aae3-16ae688fc2f7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ScrapyProfesiaSetupTest.py created successfully with the following content:\n",
            "def test_library_installation():\n",
            "    try:\n",
            "        import requests, google.colab\n",
            "    except ImportError:\n",
            "        assert False, \"Required library is not installed\"\n",
            "    assert True\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pytest ScrapyProfesiaSetupTest.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRCY-HHd9PpW",
        "outputId": "73d75715-a812-4485-91f5-8c62695596bb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m======================================= test session starts ========================================\u001b[0m\n",
            "platform linux -- Python 3.11.11, pytest-8.3.4, pluggy-1.5.0\n",
            "rootdir: /content\n",
            "plugins: langsmith-0.3.2, typeguard-4.4.1, anyio-3.7.1\n",
            "\u001b[1mcollecting ... \u001b[0m\u001b[1m\rcollected 1 item                                                                                   \u001b[0m\n",
            "\n",
            "ScrapyProfesiaSetupTest.py \u001b[32m.\u001b[0m\u001b[32m                                                                 [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m======================================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 1.29s\u001b[0m\u001b[32m =========================================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.0 Extract Raw Data"
      ],
      "metadata": {
        "id": "Sg1zYTEiPYrk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LINK = 'https://www.profesia.sk/O4988508'\n",
        "RAWFILE = '/content/ScrapyProfesiaRawData/O4988508.txt'\n",
        "PROCESSEDFILE = '/content/ScrapyProfesiaProcessedData/O4988508P.json'"
      ],
      "metadata": {
        "id": "l1NKiMPGPc7m"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Single page - proof of concept"
      ],
      "metadata": {
        "id": "fgKyGqlWPdoS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def download_and_save(url, filename):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()  # Raise an exception for bad status codes\n",
        "\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        #Convert the parsed html content to a string\n",
        "        #html_string = str(soup)\n",
        "        text_content = soup.get_text(separator='\\n', strip=True) # Get the text content with newlines as separators\n",
        "\n",
        "        # Create a dictionary to store the data\n",
        "        #data = {\"html_content\": html_string}\n",
        "\n",
        "        with open(filename, 'w', encoding='utf-8') as f:\n",
        "            f.write(text_content)\n",
        "        print(f\"Successfully downloaded and saved to {filename}\")\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error downloading URL: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n"
      ],
      "metadata": {
        "id": "CU9AFYYbPkGM"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "download_and_save(LINK, RAWFILE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ef8v8CKNURSq",
        "outputId": "69e9cc28-d2d0-4b0c-f451-14404ee64518"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully downloaded and saved to /content/ScrapyProfesiaRawData/O4988508.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.0 Transform Raw Data"
      ],
      "metadata": {
        "id": "QCuCoTJFlnoF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_data(inputfile,outputfile):\n",
        "\n",
        "  with open(inputfile, 'r', encoding='utf-8') as f:\n",
        "          text_content = f.read()\n",
        "  text_content = re.sub(r'Hľadanie práce.*?Hľadanie práce', '', text_content, flags=re.DOTALL)\n",
        "\n",
        "  if 'Odporučiť ponuku známemu' in text_content:\n",
        "    text_content = text_content.split('Odporučiť ponuku známemu', 1)[0]\n",
        "  else:\n",
        "    text_content = text_content.split('Reagovať na ponuku', 1)[0]\n",
        "  #initiate data structure\n",
        "  data = {}\n",
        "\n",
        "  #regex ID - 7 digit number - test if 7 digits\n",
        "  data['ID'] = re.search(r'ID:\\s*(\\d+)', text_content).group(1)\n",
        "\n",
        "  # data when job offer was published - month.day.year or day.month.year\n",
        "  # test ...\n",
        "  PublishedDateRaw = re.search(r'Dátum zverejnenia:\\s*([\\d\\.]+)', text_content).group(1)\n",
        "  PublishedDateObject = datetime.datetime.strptime(PublishedDateRaw, \"%d.%m.%Y\")\n",
        "  data['PublishedDate'] = PublishedDateObject.strftime(\"%Y-%m-%dT00:00:00+00:00\")\n",
        "\n",
        "  #\n",
        "  data['Location'] = re.search(r'lokalita:\\s*(.+)', text_content).group(1)\n",
        "\n",
        "  # Find text between 'Pozícia' and 'Spoločnosť'\n",
        "  positions_text = re.search(r'Pozícia:\\s*(.+?)(?=\\nSpoločnosť:)', text_content, re.DOTALL).group(1)\n",
        "\n",
        "  # Get valid lines, ignore ',', create\n",
        "  positions = [line.strip() for line in positions_text.splitlines() if line.strip() and line.strip() != ',']\n",
        "  data['Positions'] = positions\n",
        "  data['Company'] = re.search(r'Spoločnosť:\\s*(.+)', text_content).group(1)\n",
        "  data['SalaryBrutto'] = re.search(r'Základná zložka mzdy \\(brutto\\):\\s*(.+)', text_content).group(1)\n",
        "  data['JobOfferText'] = text_content\n",
        "\n",
        "  json_data = json.dumps(data, indent=4)\n",
        "\n",
        "  with open(outputfile, 'w', encoding='utf-8') as f:\n",
        "            f.write(json_data)\n",
        "  print(f'Successfully downloaded and saved to {outputfile}')"
      ],
      "metadata": {
        "id": "gyOteVgFQWyL"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extract_data(RAWFILE,PROCESSEDFILE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPM1RKbRdZML",
        "outputId": "8feea881-c2f2-4ddb-bdb0-df6f566a6d55"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully downloaded and saved to /content/ScrapyProfesiaProcessedData/O4988508P.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "# Get the public IP address\n",
        "public_ip = requests.get(\"https://api64.ipify.org\").text\n",
        "print(\"Google Colab Public IP:\", public_ip)\n",
        "#Add IP to FW@AzureBlobStorage\n",
        "#Google Colab Public IP: 34.86.192.81\n",
        "#Google Colab Public IP: 34.136.176.126\n",
        "#Not a static IP|"
      ],
      "metadata": {
        "id": "CW9Y_cJvPbOa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48bdc558-962d-4ad5-aa61-ed7d9fbbe175"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Google Colab Public IP: 34.66.47.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "connection_string = userdata.get('connectionstring')\n",
        "\n",
        "from azure.storage.blob import BlobServiceClient\n",
        "\n",
        "container_name=\"profesiafulldata\"\n",
        "blob_name = \"offer_4988508.txt\""
      ],
      "metadata": {
        "id": "V5Qs_g8UhK3Q"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_all_offers(connection_string, container_name):\n",
        "    \"\"\"\n",
        "    Reads all 'offer_*.txt' files from Azure Blob Storage, processes them using\n",
        "    extract_data_azure, and saves the output as JSON files.\n",
        "\n",
        "    Args:\n",
        "        connection_string (str): The connection string for your Azure Blob Storage account.\n",
        "        container_name (str): The name of the container containing the offer files.\n",
        "    \"\"\"\n",
        "\n",
        "    # Connect to the blob service\n",
        "    blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
        "    container_client = blob_service_client.get_container_client(container_name)\n",
        "\n",
        "    # List all blobs with the 'offer_' prefix\n",
        "    blob_list = container_client.list_blobs(prefix=\"offer_\")\n",
        "\n",
        "    for blob in blob_list:\n",
        "        if blob.name.endswith(\".txt\"):  # Process only .txt files\n",
        "            # Download the blob content\n",
        "            blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob.name)\n",
        "            blob_content = blob_client.download_blob().readall().decode(\"utf-8\")\n",
        "            metadata = blob_client.get_blob_properties()\n",
        "\n",
        "            # Construct output file name\n",
        "            output_file_name = blob.name.replace(\".txt\", \"P.json\")\n",
        "            output_file_path = f\"/content/ScrapyProfesiaProcessedData/{output_file_name}\"\n",
        "            print(output_file_path)\n",
        "\n",
        "            # Process the data and save to JSON\n",
        "            #extract_data_azure(blob_content, metadata, output_file_path)"
      ],
      "metadata": {
        "id": "JonD1-AS_5z3"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect to the blob service\n",
        "blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
        "\n",
        "# text\n",
        "blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name)\n",
        "\n",
        "# Download the blob content\n",
        "#blob_data = blob_client.download_blob()\n",
        "\n",
        "# Read data\n",
        "blob_content = blob_client.download_blob().readall().decode(\"utf-8\")"
      ],
      "metadata": {
        "id": "Qq8O0pBnimT2"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get blob properties\n",
        "# blob_properties = blob_client.get_blob_properties()"
      ],
      "metadata": {
        "id": "jPnULpxyi9-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# blob_properties"
      ],
      "metadata": {
        "id": "mpjkOgKMjQbc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ExtractDate = blob_client.get_blob_properties()\n",
        "#ExtractDate = blob_properties.get(\"creation_time\")"
      ],
      "metadata": {
        "id": "Gm93JijHtFZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#timestamp_str = ExtractDate.isoformat()"
      ],
      "metadata": {
        "id": "y9ObDwymDVvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ExtractDate"
      ],
      "metadata": {
        "id": "GSpKtWgy-Po_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#timestamp_str"
      ],
      "metadata": {
        "id": "iQ3AJ-g4Dfzu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ExtractDate1 = blob_client.get_blob_properties().get(\"creation_time\").isoformat()\n",
        "#ExtractDate1"
      ],
      "metadata": {
        "id": "cfAFTdieELtd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RAWFILEAZURE = blob_content\n",
        "METADATA = blob_client.get_blob_properties()\n",
        "PROCESSEDFILEAZURE = '/content/ScrapyProfesiaProcessedData/offer_4988508P.json'"
      ],
      "metadata": {
        "id": "qMOBzH5wjRl6"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_data_azure(inputfile,metadatafile,outputfile):\n",
        "\n",
        "  text_content = re.sub(r'Hľadanie práce.*?Hľadanie práce', '', inputfile, flags=re.DOTALL)\n",
        "\n",
        "  if 'Odporučiť ponuku známemu' in text_content:\n",
        "    text_content = text_content.split('Odporučiť ponuku známemu', 1)[0]\n",
        "  else:\n",
        "    text_content = text_content.split('Reagovať na ponuku', 1)[0]\n",
        "\n",
        "  data = {}\n",
        "  data['ID'] = re.search(r'ID:\\s*(\\d+)', text_content).group(1)\n",
        "  data['PublishedDate'] = re.search(r'Dátum zverejnenia:\\s*([\\d\\.]+)', text_content).group(1)\n",
        "  data['ExtractDate'] = metadatafile.get(\"creation_time\").isoformat()\n",
        "  data['Location'] = re.search(r'lokalita:\\s*(.+)', text_content).group(1)\n",
        "  # Find text between 'Pozícia' and 'Spoločnosť'\n",
        "  positions_text = re.search(r'Pozícia:\\s*(.+?)(?=\\nSpoločnosť:)', text_content, re.DOTALL).group(1)\n",
        "  # Get valid lines, ignore ',', create\n",
        "  positions = [line.strip() for line in positions_text.splitlines() if line.strip() and line.strip() != ',']\n",
        "  data['Positions'] = positions\n",
        "  data['Company'] = re.search(r'Spoločnosť:\\s*(.+)', text_content).group(1)\n",
        "  data['SalaryBrutto'] = re.search(r'Základná zložka mzdy \\(brutto\\):\\s*(.+)', text_content).group(1)\n",
        "  data['JobOfferText'] = text_content\n",
        "\n",
        "  json_data = json.dumps(data, indent=4)\n",
        "\n",
        "  with open(outputfile, 'w', encoding='utf-8') as f:\n",
        "            f.write(json_data)\n",
        "  print(f'Successfully downloaded and saved to {outputfile}')"
      ],
      "metadata": {
        "id": "40OGNJUusxCT"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extract_data_azure(blob_content,METADATA,PROCESSEDFILEAZURE)"
      ],
      "metadata": {
        "id": "gpc2u57Ioean",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fcc8132-8997-4e13-e675-ae0f11da90bd"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully downloaded and saved to /content/ScrapyProfesiaProcessedData/offer_4988508P.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /content/ScrapyProfesiaProcessedData/offer_4988508P.json"
      ],
      "metadata": {
        "id": "klGnqE9vtBIz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "417a474c-c1a9-4fcb-81ac-a6f86b02a81f"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"ID\": \"4988508\",\n",
            "    \"PublishedDate\": \"8.1.2025\",\n",
            "    \"ExtractDate\": \"2025-01-09T10:03:08+00:00\",\n",
            "    \"Location\": \"Bratislava\",\n",
            "    \"Positions\": [\n",
            "        \"Business Intelligence Specialist\",\n",
            "        \"Database Analyst\",\n",
            "        \"Programmer\",\n",
            "        \"Python Programmer\"\n",
            "    ],\n",
            "    \"Company\": \"Swiss Re\",\n",
            "    \"SalaryBrutto\": \"2 400 EUR/month\",\n",
            "    \"JobOfferText\": \"Data Engineer - Swiss Re | PROFESIA.SK\\n\\nData Engineer\\nAbout us\\nOur benefits\\nOpen positions\\nData Engineer\\nPlace of work\\nTwin City B, Mlynsk\\u00e9 nivy, Bratislava, Slovakia (Job with occasional home office)\\nContract type\\nfull-time\\nWage (gross)\\n2 400 - 4 100 EUR/month\\nFor Slovakia the base salary range for this position is between [EUR 2,400] and [EUR 4,100] per month (for a full-time role). The specific salary offered considers: \\u2022 the requirements, scope, complexity and responsibilities of the role, \\u2022 the applicant\\u2019s own profile including education/qualifications, expertise, specialization, skills and experience. In addition to your base salary, you may be eligible for additional rewards and benefits including an attractive performance-based bonus.\\nInformation about the position\\nJob description, responsibilities and duties\\nIn the role of Data Engineer, your core responsibility will be to maintain existing and develop new ETL transformations, process automations and reporting solutions.\\nAs a member of our team, you will:\\n\\u2022 Work with a wide range of tools and technologies \\u2022 Debug, maintain and develop SQL and Python data transformations\\n\\u2022 Perform analyses of existing code, identify problems and propose solutions\\n\\u2022 Analyze requirements, understand specifications and transform them into technical solutions\\n\\u2022 Work closely with business analysts and business stakeholders\\nAbout the team:\\nOur team is a diverse mix of 10, working in hybrid setup with a footprint in different world locations, with majority of the team located in Bratislava. The team is part of Asset Management department, and we cover various topics, including regulatory reporting, data reconciliations, data reporting and process automation.\\nEmployee perks, benefits\\nYou can look forward to extra rewards and benefits including:\\n- Attractive performance-based bonus\\n- Ultra flexible working time in hybrid setup, allowing you to work also from home\\n- Modern office spaces in attractive location\\n- 5 additional days of holiday\\n- Lunch allowance fully paid by Swiss Re\\n- Referral bonus\\n- Pension & risk insurance contribution\\n- Sick days and sick leave support\\n- Public transport benefit\\n- Multisport card\\nInformation about the selection process\\nWe provide feedback to all candidates via email. If you have not heard back from us, please check your spam folder.\\nRequirements for the employee\\nCandidates with education suit the position\\nUniversity education (Bachelor's degree)\\nUniversity education (Master's degree)\\nPostgraduate (Doctorate)\\nLanguage skills\\nEnglish - Upper intermediate (B2)\\nPersonality requirements and skills\\nYou are eager to learn to navigate and work in a wide application landscape. Your daily job is to write and debug Python and SQL code. You want to learn new things, new tools, new technologies.\\nEssentials\\n\\u2022 University degree or equivalent experience in a technical role\\n\\u2022 3+ years of relevant working experience on data transformations\\n\\u2022 Proficiency in Oracle and/or PostgreSQL\\n\\u2022 Proficiency in Python\\n\\u2022 Sound communication skills with stakeholders across an organisation, with good command of English\\nNice to Haves\\n\\u2022 Experience or willingness to learn Power BI, PowerAutomate, SSRS, Palantir's Foundry \\u2022 Experience with Jira, DevOps, Azure\\n\\u2022 Knowledge of financial products, asset management topics\\nAdvertiser\\nBrief description of the company\\nAs the world's leading and most diversified global reinsurer, we offer as our core business financial services products that enable risk taking essential to enterprise and progress. Our company was founded in Zurich, Switzerland in 1863, and operates in more than 25 countries and provides its expertise and services to clients throughout the world. We combine financial strengths with experience, knowledge and creative thought to explore new opportunities in the interests of our clients, staff and shareholders.\\nNumber of employees\\n1700 and more employees\\nCompany address\\nLearn more about working in Swiss Re Slovakia.\\nContact\\nContact person: Livia Pasztorova\\nApply here\\nID:\\n4988508\\nD\\u00e1tum zverejnenia:\\n8.1.2025\\n2025-01-08\\nlokalita:\\nBratislava\\nPoz\\u00edcia:\\nBusiness Intelligence Specialist\\n,\\nDatabase Analyst\\n,\\nProgrammer\\n,\\nPython Programmer\\nSpolo\\u010dnos\\u0165:\\nSwiss Re\\nZ\\u00e1kladn\\u00e1 zlo\\u017eka mzdy (brutto):\\n2 400 EUR/month\\n\"\n",
            "}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /content/ScrapyProfesiaProcessedData/O4988508P.json"
      ],
      "metadata": {
        "id": "gYhqbUg5N6X4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cefa8c46-58a6-4340-81de-19f278f1dabe"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"ID\": \"4988508\",\n",
            "    \"PublishedDate\": \"2025-01-08T00:00:00+00:00\",\n",
            "    \"Location\": \"Bratislava\",\n",
            "    \"Positions\": [\n",
            "        \"Business Intelligence Specialist\",\n",
            "        \"Database Analyst\",\n",
            "        \"Programmer\",\n",
            "        \"Python Programmer\"\n",
            "    ],\n",
            "    \"Company\": \"Swiss Re\",\n",
            "    \"SalaryBrutto\": \"2 400 EUR/month\",\n",
            "    \"JobOfferText\": \"Data Engineer - Swiss Re | PROFESIA.SK\\n\\nData Engineer\\nAbout us\\nOur benefits\\nOpen positions\\nData Engineer\\nPlace of work\\nTwin City B, Mlynsk\\u00e9 nivy, Bratislava, Slovakia (Job with occasional home office)\\nContract type\\nfull-time\\nWage (gross)\\n2 400 - 4 100 EUR/month\\nFor Slovakia the base salary range for this position is between [EUR 2,400] and [EUR 4,100] per month (for a full-time role). The specific salary offered considers: \\u2022 the requirements, scope, complexity and responsibilities of the role, \\u2022 the applicant\\u2019s own profile including education/qualifications, expertise, specialization, skills and experience. In addition to your base salary, you may be eligible for additional rewards and benefits including an attractive performance-based bonus.\\nInformation about the position\\nJob description, responsibilities and duties\\nIn the role of Data Engineer, your core responsibility will be to maintain existing and develop new ETL transformations, process automations and reporting solutions.\\nAs a member of our team, you will:\\n\\u2022 Work with a wide range of tools and technologies \\u2022 Debug, maintain and develop SQL and Python data transformations\\n\\u2022 Perform analyses of existing code, identify problems and propose solutions\\n\\u2022 Analyze requirements, understand specifications and transform them into technical solutions\\n\\u2022 Work closely with business analysts and business stakeholders\\nAbout the team:\\nOur team is a diverse mix of 10, working in hybrid setup with a footprint in different world locations, with majority of the team located in Bratislava. The team is part of Asset Management department, and we cover various topics, including regulatory reporting, data reconciliations, data reporting and process automation.\\nEmployee perks, benefits\\nYou can look forward to extra rewards and benefits including:\\n- Attractive performance-based bonus\\n- Ultra flexible working time in hybrid setup, allowing you to work also from home\\n- Modern office spaces in attractive location\\n- 5 additional days of holiday\\n- Lunch allowance fully paid by Swiss Re\\n- Referral bonus\\n- Pension & risk insurance contribution\\n- Sick days and sick leave support\\n- Public transport benefit\\n- Multisport card\\nInformation about the selection process\\nWe provide feedback to all candidates via email. If you have not heard back from us, please check your spam folder.\\nRequirements for the employee\\nCandidates with education suit the position\\nUniversity education (Bachelor's degree)\\nUniversity education (Master's degree)\\nPostgraduate (Doctorate)\\nLanguage skills\\nEnglish - Upper intermediate (B2)\\nPersonality requirements and skills\\nYou are eager to learn to navigate and work in a wide application landscape. Your daily job is to write and debug Python and SQL code. You want to learn new things, new tools, new technologies.\\nEssentials\\n\\u2022 University degree or equivalent experience in a technical role\\n\\u2022 3+ years of relevant working experience on data transformations\\n\\u2022 Proficiency in Oracle and/or PostgreSQL\\n\\u2022 Proficiency in Python\\n\\u2022 Sound communication skills with stakeholders across an organisation, with good command of English\\nNice to Haves\\n\\u2022 Experience or willingness to learn Power BI, PowerAutomate, SSRS, Palantir's Foundry \\u2022 Experience with Jira, DevOps, Azure\\n\\u2022 Knowledge of financial products, asset management topics\\nAdvertiser\\nBrief description of the company\\nAs the world's leading and most diversified global reinsurer, we offer as our core business financial services products that enable risk taking essential to enterprise and progress. Our company was founded in Zurich, Switzerland in 1863, and operates in more than 25 countries and provides its expertise and services to clients throughout the world. We combine financial strengths with experience, knowledge and creative thought to explore new opportunities in the interests of our clients, staff and shareholders.\\nNumber of employees\\n1700 and more employees\\nCompany address\\nLearn more about working in Swiss Re Slovakia.\\nContact\\nContact person: Livia Pasztorova\\nApply here\\nID:\\n4988508\\nD\\u00e1tum zverejnenia:\\n8.1.2025\\n2025-01-08\\nlokalita:\\nBratislava\\nPoz\\u00edcia:\\nBusiness Intelligence Specialist\\n,\\nDatabase Analyst\\n,\\nProgrammer\\n,\\nPython Programmer\\nSpolo\\u010dnos\\u0165:\\nSwiss Re\\nZ\\u00e1kladn\\u00e1 zlo\\u017eka mzdy (brutto):\\n2 400 EUR/month\\n\"\n",
            "}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -aGl /content/ScrapyProfesiaProcessedData/"
      ],
      "metadata": {
        "id": "vb6t_kjGOJey",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39152bb9-2d94-4d61-833f-f34643234504"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 24\n",
            "drwxr-xr-x 2 root 4096 Feb  3 12:37 .\n",
            "drwxr-xr-x 1 root 4096 Feb  3 12:30 ..\n",
            "-rw-r--r-- 1 root 4803 Feb  3 12:33 O4988508P.json\n",
            "-rw-r--r-- 1 root 4834 Feb  3 12:37 offer_4988508P.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!ping 95.105.175.243"
      ],
      "metadata": {
        "id": "r9uQCjIDOOTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import requests\n",
        "\n",
        "#try:\n",
        "#    response = requests.get('http://95.105.175.243')  # Replace with your IP address\n",
        "#    if response.status_code == 200:\n",
        "#        print(\"The IP is reachable.\")\n",
        "#    else:\n",
        "#        print(f\"Server responded with status code: {response.status_code}\")\n",
        "#except requests.exceptions.RequestException as e:\n",
        "#    print(f\"Error connecting to the IP: {e}\")\n"
      ],
      "metadata": {
        "id": "KKP5dPEjL0yU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "imk2d1aFM-6g"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
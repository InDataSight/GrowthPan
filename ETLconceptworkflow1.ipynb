{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "3iJuNSZ2N0tK",
        "Sg1zYTEiPYrk",
        "fgKyGqlWPdoS",
        "QCuCoTJFlnoF"
      ],
      "authorship_tag": "ABX9TyNRoIGYY5VMusPB6b0uVk9r",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/InDataSight/GrowthPan/blob/main/ETLconceptworkflow1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.0 Setup"
      ],
      "metadata": {
        "id": "3iJuNSZ2N0tK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Folder structure"
      ],
      "metadata": {
        "id": "1CzvuVMaOpNF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "aHUVrvZUydHy"
      },
      "outputs": [],
      "source": [
        "#!mkdir ScrapyProfesiaRawData ScrapyProfesiaProcessedData ScrapyProfesiaLogs"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Install libraries and modules"
      ],
      "metadata": {
        "id": "I_oTZca_OwCe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ox5X4Ff3RwyK",
        "outputId": "81ab2ed9-d48b-425d-d85a-1da1dde7ceb2"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bs4 (from -r requirements.txt (line 1))\n",
            "  Using cached bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from bs4->-r requirements.txt (line 1)) (4.12.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->bs4->-r requirements.txt (line 1)) (2.6)\n",
            "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
            "Installing collected packages: bs4\n",
            "Successfully installed bs4-0.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "import re\n",
        "\n",
        "#Run pip for defined modules in the requirements.txt\n",
        "pip_install_result = subprocess.run(['pip', 'install', '-r', 'requirements.txt'], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "\n",
        "#output results for the pip_install_result\n",
        "if pip_install_result.returncode == 0:\n",
        "    print(\"Pip install successful\")\n",
        "else:\n",
        "    print(\"Pip install failed\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eormRimU9LXq",
        "outputId": "36d3c7cb-485d-412d-ebb8-21f8e20a77ab"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pip install successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3 Test"
      ],
      "metadata": {
        "id": "SAgf05ZNOzBe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pytest ScrapyProfesiaSetupTest.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRCY-HHd9PpW",
        "outputId": "fa343d0e-0b06-4560-b609-0ee1fc79deaa"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m======================================= test session starts ========================================\u001b[0m\n",
            "platform linux -- Python 3.11.11, pytest-8.3.4, pluggy-1.5.0\n",
            "rootdir: /content\n",
            "plugins: typeguard-4.4.1, anyio-3.7.1\n",
            "\u001b[1mcollecting ... \u001b[0m\u001b[1m\rcollected 2 items                                                                                  \u001b[0m\n",
            "\n",
            "ScrapyProfesiaSetupTest.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m======================================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.43s\u001b[0m\u001b[32m =========================================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.0 Extract Raw Data"
      ],
      "metadata": {
        "id": "Sg1zYTEiPYrk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LINK = 'https://www.profesia.sk/O4988508'\n",
        "RAWFILE = '/content/ScrapyProfesiaRawData/O4988508.txt'\n",
        "PROCESSEDFILE = '/content/ScrapyProfesiaProcessedData/O4988508P.json'"
      ],
      "metadata": {
        "id": "l1NKiMPGPc7m"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Single page - proof of concept"
      ],
      "metadata": {
        "id": "fgKyGqlWPdoS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def download_and_save(url, filename):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()  # Raise an exception for bad status codes\n",
        "\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        #Convert the parsed html content to a string\n",
        "        #html_string = str(soup)\n",
        "        text_content = soup.get_text(separator='\\n', strip=True) # Get the text content with newlines as separators\n",
        "\n",
        "        # Create a dictionary to store the data\n",
        "        #data = {\"html_content\": html_string}\n",
        "\n",
        "        with open(filename, 'w', encoding='utf-8') as f:\n",
        "            f.write(text_content)\n",
        "        print(f\"Successfully downloaded and saved to {filename}\")\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error downloading URL: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n"
      ],
      "metadata": {
        "id": "CU9AFYYbPkGM"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "download_and_save(LINK, RAWFILE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ef8v8CKNURSq",
        "outputId": "66c763c1-430f-451d-90e1-afcba67aeed2"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully downloaded and saved to /content/ScrapyProfesiaRawData/O4988508.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.0 Transform Raw Data"
      ],
      "metadata": {
        "id": "QCuCoTJFlnoF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_data(inputfile,outputfile):\n",
        "\n",
        "  with open(inputfile, 'r', encoding='utf-8') as f:\n",
        "          text_content = f.read()\n",
        "  text_content = re.sub(r'Hľadanie práce.*?Hľadanie práce', '', text_content, flags=re.DOTALL)\n",
        "\n",
        "  if 'Odporučiť ponuku známemu' in text_content:\n",
        "    text_content = text_content.split('Odporučiť ponuku známemu', 1)[0]\n",
        "  else:\n",
        "    text_content = text_content.split('Reagovať na ponuku', 1)[0]\n",
        "\n",
        "  data = {}\n",
        "  data['ID'] = re.search(r'ID:\\s*(\\d+)', text_content).group(1)\n",
        "  data['PublishedDate'] = re.search(r'Dátum zverejnenia:\\s*([\\d\\.]+)', text_content).group(1)\n",
        "  #add data['ExtractDate'] = ... wont work if I dont have metadata available\n",
        "  #at blob storage - test\n",
        "  #at VM file extract date is the date I am looking for\n",
        "  #should be almost same as the blob Creation-Date\n",
        "  data['Location'] = re.search(r'lokalita:\\s*(.+)', text_content).group(1)\n",
        "  # Find text between 'Pozícia' and 'Spoločnosť'\n",
        "  positions_text = re.search(r'Pozícia:\\s*(.+?)(?=\\nSpoločnosť:)', text_content, re.DOTALL).group(1)\n",
        "  # Get valid lines, ignore ',', create\n",
        "  positions = [line.strip() for line in positions_text.splitlines() if line.strip() and line.strip() != ',']\n",
        "  data['Positions'] = positions\n",
        "  data['Company'] = re.search(r'Spoločnosť:\\s*(.+)', text_content).group(1)\n",
        "  data['SalaryBrutto'] = re.search(r'Základná zložka mzdy \\(brutto\\):\\s*(.+)', text_content).group(1)\n",
        "  data['JobOfferText'] = text_content\n",
        "\n",
        "  json_data = json.dumps(data, indent=4)\n",
        "\n",
        "  with open(outputfile, 'w', encoding='utf-8') as f:\n",
        "            f.write(json_data)\n",
        "  print(f'Successfully downloaded and saved to {outputfile}')"
      ],
      "metadata": {
        "id": "gyOteVgFQWyL"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extract_data(RAWFILE,PROCESSEDFILE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPM1RKbRdZML",
        "outputId": "7fb783d0-6309-400d-cb75-b7e1fe1963dd"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully downloaded and saved to /content/ScrapyProfesiaProcessedData/O4988508P.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OPW_zzqWgQnf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}